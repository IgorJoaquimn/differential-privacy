{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0851e9",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e89ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/priv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports e setup\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c05d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Avalia o modelo em um conjunto de dados e retorna as métricas de acurácia e F1-score\n",
    "    \"\"\"\n",
    "    # Certifique-se de que o modelo esteja em modo de avaliação\n",
    "    model.eval()\n",
    "\n",
    "    # Coletar todas as previsões e rótulos reais\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "    # Calcular métricas\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')  # usa média ponderada por classe\n",
    "\n",
    "    print(f\"Acurácia: {acc:.4f}\")\n",
    "    print(f\"F1-score (weighted): {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732a80e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31678/2807996909.py:2: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"dataPrep/data/movies_tokenized.csv\")  # Certifique-se de que esse arquivo existe\n"
     ]
    }
   ],
   "source": [
    "# 2. Carregamento dos dados tokenizados\n",
    "df = pd.read_csv(\"dataPrep/data/movies_tokenized.csv\")  # Certifique-se de que esse arquivo existe\n",
    "\n",
    "# Extraindo apenas o primeiro gênero como rótulo (ou 'Unknown' se vazio)\n",
    "df['genre'] = df['genres'].fillna(\"[]\").apply(lambda x: eval(x)[0]['name'] if eval(x) else 'Unknown')\n",
    "\n",
    "# Converter a string dos tokens para lista de inteiros\n",
    "df['overview_tokens'] = df['overview_tokens'].apply(lambda x: eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Remover linhas com tokens vazios\n",
    "df = df[df['overview_tokens'].apply(len) > 0]\n",
    "\n",
    "# Codificar os gêneros em inteiros\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd96e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset PyTorch\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, token_ids, labels, max_len=128):\n",
    "        self.token_ids = token_ids\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.token_ids[idx][:self.max_len]\n",
    "        x = x + [0] * (self.max_len - len(x))  # Padding manual\n",
    "        return torch.tensor(x), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Criar o dataset e dataloader\n",
    "dataset = MovieDataset(df['overview_tokens'].tolist(), df['label'].tolist())\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2551ce",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c713a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Modelo simples: embedding + média + linear\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)              # [B, T, D]\n",
    "        pooled = embedded.mean(dim=1)             # [B, D]\n",
    "        return self.fc(pooled)\n",
    "\n",
    "# Tamanho do vocabulário e número de classes\n",
    "vocab_size = max([max(seq) for seq in df['overview_tokens']]) + 1\n",
    "num_classes = df['label'].nunique()\n",
    "\n",
    "# Instanciar o modelo\n",
    "model = TextClassifier(vocab_size, 128, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db814fa",
   "metadata": {},
   "source": [
    "# DP-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d28e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/priv/lib/python3.11/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5. Treinamento com DP-SGD (Opacus)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Attach o PrivacyEngine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, dataloader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=dataloader,\n",
    "    noise_multiplier=1.0,\n",
    "    max_grad_norm=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8dcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de treino\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/priv/lib/python3.11/site-packages/opacus/accountants/prv.py:151: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mesh_size = eps_error / np.sqrt(\n",
      "/home/gabriel/miniconda3/envs/priv/lib/python3.11/site-packages/opacus/accountants/analysis/prv/domain.py:43: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  t_min = np.floor(t_min / dt) * dt\n",
      "/home/gabriel/miniconda3/envs/priv/lib/python3.11/site-packages/opacus/accountants/analysis/prv/domain.py:44: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  t_max = np.ceil(t_max / dt) * dt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6. Cálculo do epsilon após o treino\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m epsilon = \u001b[43mprivacy_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccountant\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_epsilon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mε = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, δ = 1e-5\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/priv/lib/python3.11/site-packages/opacus/accountants/prv.py:102\u001b[39m, in \u001b[36mPRVAccountant.get_epsilon\u001b[39m\u001b[34m(self, delta, eps_error, delta_error, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m     delta_error = delta / \u001b[32m1000\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# we construct a discrete PRV from the history\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m dprv = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_dprv\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelta_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# this discrete PRV can be used to directly estimate and bound epsilon\u001b[39;00m\n\u001b[32m    104\u001b[39m _, _, eps_upper = dprv.compute_epsilon(delta, delta_error, eps_error)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/priv/lib/python3.11/site-packages/opacus/accountants/prv.py:119\u001b[39m, in \u001b[36mPRVAccountant._get_dprv\u001b[39m\u001b[34m(self, eps_error, delta_error)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# compute a safe domain for discretization per Gopi et al. This determines both\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# the mesh size and the truncation upper and lower bounds.\u001b[39;00m\n\u001b[32m    118\u001b[39m num_self_compositions = [steps \u001b[38;5;28;01mfor\u001b[39;00m _, _, steps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.history]\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m domain = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_domain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_self_compositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_self_compositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelta_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelta_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m tprvs = [\n\u001b[32m    126\u001b[39m     TruncatedPrivacyRandomVariable(prv, domain.t_min, domain.t_max)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prv \u001b[38;5;129;01min\u001b[39;00m prvs\n\u001b[32m    128\u001b[39m ]\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# discretize and convolve prvs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/priv/lib/python3.11/site-packages/opacus/accountants/prv.py:155\u001b[39m, in \u001b[36mPRVAccountant._get_domain\u001b[39m\u001b[34m(self, prvs, num_self_compositions, eps_error, delta_error)\u001b[39m\n\u001b[32m    144\u001b[39m L = compute_safe_domain_size(\n\u001b[32m    145\u001b[39m     prvs=prvs,\n\u001b[32m    146\u001b[39m     max_self_compositions=num_self_compositions,\n\u001b[32m    147\u001b[39m     eps_error=eps_error,\n\u001b[32m    148\u001b[39m     delta_error=delta_error,\n\u001b[32m    149\u001b[39m )\n\u001b[32m    151\u001b[39m mesh_size = eps_error / np.sqrt(\n\u001b[32m    152\u001b[39m     total_self_compositions * np.log(\u001b[32m12\u001b[39m / delta_error) / \u001b[32m2\u001b[39m\n\u001b[32m    153\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDomain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_aligned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/priv/lib/python3.11/site-packages/opacus/accountants/analysis/prv/domain.py:46\u001b[39m, in \u001b[36mDomain.create_aligned\u001b[39m\u001b[34m(cls, t_min, t_max, dt)\u001b[39m\n\u001b[32m     43\u001b[39m t_min = np.floor(t_min / dt) * dt\n\u001b[32m     44\u001b[39m t_max = np.ceil(t_max / dt) * dt\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m size = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_max\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_min\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m + \u001b[32m1\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size % \u001b[32m2\u001b[39m == \u001b[32m1\u001b[39m:\n\u001b[32m     49\u001b[39m     size += \u001b[32m1\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# 6. Cálculo do epsilon após o treino E avaliação do modelo\n",
    "epsilon = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
    "print(f\"ε = {epsilon:.2f}, δ = 1e-5\")\n",
    "evaluate_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83552006",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34d57d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m     loss.backward()\n\u001b[32m     17\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[BASELINE] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- BASELINE SEM PRIVACIDADE ---\n",
    "from torch.optim import Adam\n",
    "\n",
    "model_baseline = TextClassifier(vocab_size, 128, num_classes).to(device)\n",
    "optimizer = Adam(model_baseline.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_baseline.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_baseline(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[BASELINE] Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0c0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.4552\n",
      "F1-score (weighted): 0.3749\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_baseline, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e414ca",
   "metadata": {},
   "source": [
    "# Embedding Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ed394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_embeddings(embedding_layer, sigma=0.1):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(embedding_layer.weight) * sigma\n",
    "        embedding_layer.weight.add_(noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EMBED NOISE] Epoch 1 - Loss: 3374.7267\n",
      "[EMBED NOISE] Epoch 2 - Loss: 3036.5760\n",
      "[EMBED NOISE] Epoch 3 - Loss: 2778.3562\n"
     ]
    }
   ],
   "source": [
    "# --- EMBEDDING PERTURBATION ---\n",
    "model_embed = TextClassifier(vocab_size, 128, num_classes).to(device)\n",
    "\n",
    "# Aplica ruído antes do treino\n",
    "add_noise_to_embeddings(model_embed.embedding, sigma=0.1)\n",
    "\n",
    "optimizer = Adam(model_embed.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_embed.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_embed(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[EMBED NOISE] Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055b7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.4502\n",
      "F1-score (weighted): 0.3667\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_embed, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ε ≈ 1.4534 (para σ = 10, δ = 1e-05, T = 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parâmetros\n",
    "sigma = 0.1          # mesmo que você usou ao aplicar ruído\n",
    "delta = 1e-5\n",
    "sensitivity = 1.0    # padrão\n",
    "\n",
    "# Fórmula para mecanismo Gaussiano (epsilon aproximado)\n",
    "epsilon = (np.sqrt(2 * np.log(1.25 / delta)) * sensitivity) / sigma\n",
    "\n",
    "# Se aplicou ruído T vezes (ex: por época), multiplique:\n",
    "T = 3  # ou 3, se adicionou ruído por época\n",
    "epsilon_total = epsilon * T\n",
    "\n",
    "print(f\"ε ≈ {epsilon_total:.4f} (para σ = {sigma}, δ = {delta}, T = {T})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74fe9f",
   "metadata": {},
   "source": [
    "# TEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tem = TextClassifier(vocab_size, 128, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d056dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEM aplicado com sigma = 1.0\n"
     ]
    }
   ],
   "source": [
    "def apply_tem_noise(embedding_layer: torch.nn.Embedding, sigma: float = 0.1):\n",
    "    \"\"\"\n",
    "    Aplica ruído gaussiano diretamente na camada de embeddings.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(embedding_layer.weight) * sigma\n",
    "        embedding_layer.weight.add_(noise)\n",
    "\n",
    "# --- Aplicar TEM antes do treino ---\n",
    "sigma_tem = 1.0  # ajuste para obter melhor privacidade (ε ↓)\n",
    "apply_tem_noise(model_tem.embedding, sigma=sigma_tem)\n",
    "print(f\"TEM aplicado com sigma = {sigma_tem}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEM] Epoch 1 - Loss: 3390.7705\n",
      "[TEM] Epoch 2 - Loss: 3147.3419\n",
      "[TEM] Epoch 3 - Loss: 2942.7239\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_tem.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_tem.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_tem(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[TEM] Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.4174\n",
      "F1-score (weighted): 0.3331\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_tem, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea323fe",
   "metadata": {},
   "source": [
    "# MADLIB - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44cb7d",
   "metadata": {},
   "source": [
    "# DP-SGD + TEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bdfed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMBO] TEM aplicado com σ = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Instancia o modelo\n",
    "model_combo = TextClassifier(vocab_size, 128, num_classes).to(device)\n",
    "\n",
    "# --- Aplica ruído TEM às embeddings ---\n",
    "sigma_tem = 1.0\n",
    "def apply_tem_noise(embedding_layer: torch.nn.Embedding, sigma: float = 0.1):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(embedding_layer.weight) * sigma\n",
    "        embedding_layer.weight.add_(noise)\n",
    "\n",
    "apply_tem_noise(model_combo.embedding, sigma=sigma_tem)\n",
    "print(f\"[COMBO] TEM aplicado com σ = {sigma_tem}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea9981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/priv/lib/python3.11/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMBO] Epoch 1 - Loss: 3618.0475\n",
      "[COMBO] Epoch 2 - Loss: 3465.2856\n",
      "[COMBO] Epoch 3 - Loss: 3453.5662\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_combo.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Configura o PrivacyEngine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model_combo, optimizer, dataloader_combo = privacy_engine.make_private(\n",
    "    module=model_combo,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=dataloader,\n",
    "    noise_multiplier=1.0,      # σ do DP-SGD\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Treinamento\n",
    "model_combo.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader_combo:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_combo(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[COMBO] Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24822327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMBO] ε (DP-SGD): 0.22 | δ = 1e-5\n",
      "[COMBO] TEM aplicado com σ = 1.0 (ε estimado separadamente ≈ 4.84)\n",
      "Acurácia: 0.2633\n",
      "F1-score (weighted): 0.1101\n"
     ]
    }
   ],
   "source": [
    "epsilon_combo = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
    "print(f\"[COMBO] ε (DP-SGD): {epsilon_combo:.2f} | δ = 1e-5\")\n",
    "print(f\"[COMBO] TEM aplicado com σ = {sigma_tem} (ε estimado separadamente ≈ {4.84 if sigma_tem==1.0 else 'recalcular'})\")\n",
    "evaluate_model(model_combo, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
