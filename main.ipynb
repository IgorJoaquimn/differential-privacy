{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0851e9",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e89ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports e setup\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you desire reproducibility, run the following function:\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the random seed for reproducibility across various runs.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import random\n",
    "    import numpy as np\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True # Makes training slower but ensures reproducibility\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c05d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Avalia o modelo em um conjunto de dados e retorna as métricas de acurácia e F1-score\n",
    "    \"\"\"\n",
    "    # Certifique-se de que o modelo esteja em modo de avaliação\n",
    "    model.eval()\n",
    "\n",
    "    # Coletar todas as previsões e rótulos reais\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, att_mask in tqdm(dataloader, desc=\"epoch\"):\n",
    "            x, y, att_mask = x.to(device), y.to(device), att_mask.to(device)\n",
    "            logits = model(x, attention_mask=att_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "    # Calcular métricas\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')  # usa média ponderada por classe\n",
    "\n",
    "    print(f\"Acurácia: {acc:.4f}\")\n",
    "    print(f\"F1-score (weighted): {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd96e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset  import MovieDataset  # Certifique-se de que o arquivo dataset.py está no mesmo diretório\n",
    "\n",
    "# Criar o dataset e dataloader\n",
    "dataset = MovieDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2551ce",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.baseline import BaselineModel\n",
    "model = BaselineModel(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a4a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  23%|██▎       | 146/625 [00:20<01:07,  7.12it/s]"
     ]
    }
   ],
   "source": [
    "# Treinamento do modelo por 10 épocas usando o dataloader já definido\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for x, y, att_mask in tqdm(dataloader, desc=\"epoch\"):\n",
    "        x, y, att_mask = x.to(device), y.to(device), att_mask.to(device)\n",
    "        logits = model(x, attention_mask=att_mask)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "    evaluate_model(model,dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db814fa",
   "metadata": {},
   "source": [
    "# DP-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d28e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Treinamento com DP-SGD (Opacus)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Attach o PrivacyEngine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, dataloader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=dataloader,\n",
    "    noise_multiplier=1.0,\n",
    "    max_grad_norm=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8dcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de treino\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Cálculo do epsilon após o treino E avaliação do modelo\n",
    "epsilon = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
    "print(f\"ε = {epsilon:.2f}, δ = 1e-5\")\n",
    "evaluate_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83552006",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BASELINE SEM PRIVACIDADE ---\n",
    "from torch.optim import Adam\n",
    "\n",
    "model_baseline = TextClassifier(BaselineModel(),embedding_dim, num_classes).to(device)\n",
    "optimizer = Adam(model_baseline.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_baseline.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_baseline(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[BASELINE] Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_baseline, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e414ca",
   "metadata": {},
   "source": [
    "# Embedding Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ed394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_embeddings(embedding_layer, sigma=0.1):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(embedding_layer.weight) * sigma\n",
    "        embedding_layer.weight.add_(noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EMBEDDING PERTURBATION ---\n",
    "model_embed = TextClassifier(BaselineModel(),embedding_dim, num_classes).to(device)\n",
    "\n",
    "# Aplica ruído antes do treino\n",
    "add_noise_to_embeddings(model_embed.embedding, sigma=0.1)\n",
    "\n",
    "optimizer = Adam(model_embed.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_embed.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_embed(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[EMBED NOISE] Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_embed, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parâmetros\n",
    "sigma = 0.1          # mesmo que você usou ao aplicar ruído\n",
    "delta = 1e-5\n",
    "sensitivity = 1.0    # padrão\n",
    "\n",
    "# Fórmula para mecanismo Gaussiano (epsilon aproximado)\n",
    "epsilon = (np.sqrt(2 * np.log(1.25 / delta)) * sensitivity) / sigma\n",
    "\n",
    "# Se aplicou ruído T vezes (ex: por época), multiplique:\n",
    "T = 3  # ou 3, se adicionou ruído por época\n",
    "epsilon_total = epsilon * T\n",
    "\n",
    "print(f\"ε ≈ {epsilon_total:.4f} (para σ = {sigma}, δ = {delta}, T = {T})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74fe9f",
   "metadata": {},
   "source": [
    "# TEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tem import TEMModel\n",
    "model_tem = TextClassifier(TEMModel(),embedding_dim, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d056dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tem_noise(embedding_layer: torch.nn.Embedding, sigma: float = 0.1):\n",
    "    \"\"\"\n",
    "    Aplica ruído gaussiano diretamente na camada de embeddings.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(embedding_layer.weight) * sigma\n",
    "        embedding_layer.weight.add_(noise)\n",
    "\n",
    "# --- Aplicar TEM antes do treino ---\n",
    "sigma_tem = 1.0  # ajuste para obter melhor privacidade (ε ↓)\n",
    "apply_tem_noise(model_tem.embedding, sigma=sigma_tem)\n",
    "print(f\"TEM aplicado com sigma = {sigma_tem}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_tem.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_tem.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_tem(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[TEM] Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_tem, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea323fe",
   "metadata": {},
   "source": [
    "# MADLIB - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa269b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.madlib import MadlibModel\n",
    "\n",
    "model = TextClassifier(MadlibModel(epsilon=5),embedding_dim, num_classes).to(device)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44cb7d",
   "metadata": {},
   "source": [
    "# DP-SGD + TEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bdfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o modelo\n",
    "model_combo = TextClassifier(TEMModel(),embedding_dim, num_classes).to(device)\n",
    "\n",
    "# --- Aplica ruído TEM às embeddings ---\n",
    "sigma_tem = 1.0\n",
    "def apply_tem_noise(embedding_layer: torch.nn.Embedding, sigma: float = 0.1):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(embedding_layer.weight) * sigma\n",
    "        embedding_layer.weight.add_(noise)\n",
    "\n",
    "apply_tem_noise(model_combo.embedding, sigma=sigma_tem)\n",
    "print(f\"[COMBO] TEM aplicado com σ = {sigma_tem}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_combo.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Configura o PrivacyEngine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model_combo, optimizer, dataloader_combo = privacy_engine.make_private(\n",
    "    module=model_combo,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=dataloader,\n",
    "    noise_multiplier=1.0,      # σ do DP-SGD\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Treinamento\n",
    "model_combo.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader_combo:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_combo(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[COMBO] Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24822327",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_combo = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
    "print(f\"[COMBO] ε (DP-SGD): {epsilon_combo:.2f} | δ = 1e-5\")\n",
    "print(f\"[COMBO] TEM aplicado com σ = {sigma_tem} (ε estimado separadamente ≈ {4.84 if sigma_tem==1.0 else 'recalcular'})\")\n",
    "evaluate_model(model_combo, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "name_generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
